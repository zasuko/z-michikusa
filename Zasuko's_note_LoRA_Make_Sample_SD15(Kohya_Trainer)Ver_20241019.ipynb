{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmCPmqFL6hCQ"
      },
      "source": [
        "# ğŸ€ã‚ªãƒªã‚¸ãƒŠãƒ«LoRA ã®ä½œæˆ_ğŸŒ±ãŠè©¦ã—ã‚µãƒ³ãƒ—ãƒ«ç‰ˆï¼ˆSD1.5ç‰ˆï¼‰\n",
        "### â€»Lora Trainer by Zasuko Michikusa\n",
        "ğŸŒ±This is based on the work of [Kohya-ss](https://github.com/kohya-ss/sd-scripts) and [Linaqruf](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb).and [Hollowstrawberry](https://github.com/hollowstrawberry/kohya-colab) Thank you!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXg7j5p-DW4p",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # ğŸŒï¼œStep1ï¼ Google Driveã¨æ¥ç¶š\n",
        "\n",
        "#@markdown # ğŸ¥šã¯ã˜ã‚ã«ã‚„ã‚‹ã“ã¨âŠâ·\n",
        "#@markdown #ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
        "#@markdown #âŠğŸ£ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–ã«éšå±¤ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ§‹æˆ\n",
        "#@markdown ### ã€€ãƒ»äºˆã‚ä¸‹è¨˜ğŸ€ãƒãƒ¼ã‚¯ã®ãƒ•ã‚©ãƒ«ãƒ€ã¨å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”¨æ„ã—ã¾ã—ã‚‡ã†ï¼\n",
        "#@markdown ###ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½\n",
        "#@markdown ###ï¼œğŸŒGoogleDriveï¼\n",
        "#@markdown ###ğŸ“ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\n",
        "#@markdown ###â””ğŸ“checkpointã€€ã€€ã€€â† ğŸ€ãƒ™ãƒ¼ã‚¹ã«ä½¿ç”¨ã™ã‚‹Checkpointãƒ¢ãƒ‡ãƒ«ã‚’æ ¼ç´\n",
        "#@markdown ###â””ğŸ“train_imgã€€ã€€ã€€ â† ğŸ€å­¦ç¿’ç”¨ã®ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’æ ¼ç´\n",
        "#@markdown ###â””ğŸ“Loras ã€€ã€€ã€€ã€€ã€€â†(â€»1)ã€€ä½œæˆã™ã‚‹LoRAãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå˜ä½ã§ãƒ•ã‚©ãƒ«ãƒ€ç®¡ç†ã—ã¾ã™\n",
        "#@markdown ###ï¼¿â””ğŸ“project_nameã€€â†(â€»2)ã€€â·ã§å…¥åŠ›ã—ãŸåå‰ã«ãªã‚‹ï¼ˆå®Œæˆã—ãŸLoRAãƒ•ã‚¡ã‚¤ãƒ«ã®åå‰ã«ã‚‚ãªã‚‹ï¼‰\n",
        "#@markdown ###ï¼¿ï¼¿â””ğŸ“outputã€€ã€€ã€€â† (â€»3)ã€€å®Œæˆã—ãŸ LoRAãƒ•ã‚¡ã‚¤ãƒ« ãŒæ ¼ç´ã•ã‚Œã‚‹\n",
        "#@markdown ###ï¼¿ï¼¿â””ğŸ“datasetã€€  ã€€ â† (â€»4)ã€€å­¦ç¿’ã«ä½¿ã†ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’å…¥ã‚Œã‚‹ãƒ•ã‚©ãƒ«ãƒ€\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import re\n",
        "import toml\n",
        "from time import time\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "# ã“ã‚Œã‚‰ã¯éå»ã®å®Ÿè¡Œã‹ã‚‰ã®æƒ…å ±ã‚’ä¿æŒã—ã¾ã™\n",
        "if \"model_url\" in globals():\n",
        "    old_model_url = model_url\n",
        "else:\n",
        "    old_model_url = None\n",
        "if \"dependencies_installed\" not in globals():\n",
        "    dependencies_installed = False\n",
        "if \"model_file\" not in globals():\n",
        "    model_file = None\n",
        "\n",
        "# ä»–ã®ã‚»ãƒ«ã«ã‚ˆã£ã¦è¨­å®šã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹å¤‰æ•°ã€‚ã“ã‚Œã‚‰ã¯ãƒ¬ã‚¬ã‚·ãƒ¼ãªå¤‰æ•°ã§ã™ã€‚\n",
        "if \"custom_dataset\" not in globals():\n",
        "    custom_dataset = None\n",
        "if \"override_dataset_config_file\" not in globals():\n",
        "    override_dataset_config_file = None\n",
        "if \"override_config_file\" not in globals():\n",
        "    override_config_file = None\n",
        "if \"optimizer\" not in globals():\n",
        "    optimizer = \"AdamW8bit\"\n",
        "if \"optimizer_args\" not in globals():\n",
        "    optimizer_args = None\n",
        "if \"continue_from_lora\" not in globals():\n",
        "    continue_from_lora = \"\"\n",
        "if \"weighted_captions\" not in globals():\n",
        "    weighted_captions = False\n",
        "if \"adjust_tags\" not in globals():\n",
        "    adjust_tags = False\n",
        "if \"keep_tokens_weight\" not in globals():\n",
        "    keep_tokens_weight = 1.0\n",
        "\n",
        "COLAB = True  # ä½RAMãƒ¢ãƒ¼ãƒ‰ã®ãŸã‚ã®è¨­å®š\n",
        "XFORMERS = True\n",
        "SOURCE = \"https://github.com/uYouUs/sd-scripts\"\n",
        "COMMIT = None\n",
        "BETTER_EPOCH_NAMES = True\n",
        "LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "#@markdown ###ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½\n",
        "#@markdown #ã€è©³ã—ã„èª¬æ˜ã€‘\n",
        "#@markdown ####ğŸ’¡ã€ŒğŸ“Lorasã€ä»¥ä¸‹ã¯ğŸŒï¼œStep1ï¼ã®å‡¦ç†å¾Œã«è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã™\n",
        "#@markdown ##### â€»1 åå‰ã‚’ä»˜ã‘ãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå˜ä½ã§è‡ªå‹•çš„ã«ãƒ•ã‚©ãƒ«ãƒ€ç®¡ç†ã—ã¾ã™\n",
        "#@markdown ##### â€»2ã€Œproject_nameã€ãƒ•ã‚©ãƒ«ãƒ€åã‚’ä»˜ã‘ã¦ä¸‹è¨˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’æ ¼ç´\n",
        "#@markdown ##### â€»3ã€Œoutputã€ãƒ•ã‚©ãƒ«ãƒ€ã ã‘ä½œæˆã—ã¦ãŠã\n",
        "#@markdown ##### â€»4ã€Œdatesetã€ã«å­¦ç¿’ç”¨ç”»åƒã¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ ¼ç´\n",
        "#@markdown ##### ãƒ»å­¦ç¿’ç”¨ç”»åƒã®ç”»åƒæšæ•°ã¯ã€Œ20æšï½50æšã€ï¼ˆè§£åƒåº¦ï¼š512Ã—512ï¼‰\n",
        "#@markdown ##### ãƒ»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€äºˆã‚ã€Œdataset-tag-editorã€ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚\n",
        "\n",
        "#@markdown #ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼\n",
        "\n",
        "#@markdown #â·ğŸ”ä½œæˆã™ã‚‹ã€ŒLoRAåã€ã®è¨­å®š\n",
        "#@markdown ### ã€€ãƒ»è‡ªä½œã™ã‚‹ã€ŒLoLAåã€ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ï¼ˆã€Œ.ã€ã‚¹ãƒšãƒ¼ã‚¹ç­‰ã¯ä½¿ç”¨ä¸å¯ï¼‰\n",
        "project_name = \"ã“ã“ã«ä»»æ„ã®LoRAã®åå‰ã‚’å…¥åŠ›\"  # @param {type:\"string\"}\n",
        "project_name = project_name.strip()\n",
        "\n",
        "folder_structure = \"ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«æ•´ç† (MyDrive/Loras/project_name/dataset)\"  # @param [\"ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«æ•´ç† (MyDrive/Loras/project_name/dataset)\"]\n",
        "#@markdown #ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
        "\n",
        "#@markdown â€»Stable Diffusion 2ç³»ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ãƒã‚§ãƒƒã‚¯ã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚\n",
        "custom_model_is_based_on_sd2 = False  # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "def create_folders():\n",
        "    # Google DriveãŒãƒã‚¦ãƒ³ãƒˆã•ã‚Œã¦ã„ãªã„å ´åˆã¯ãƒã‚¦ãƒ³ãƒˆ\n",
        "    if COLAB and not os.path.exists('/content/drive'):\n",
        "        from google.colab import drive\n",
        "        print(\"ğŸ“‚ Google Driveã«æ¥ç¶šä¸­...\")\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # ä½œæˆã—ãŸã„ãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‘ã‚¹ã‚’ãƒªã‚¹ãƒˆã§æŒ‡å®š\n",
        "    folders_to_create = [\n",
        "        f'/content/drive/MyDrive/Loras/{project_name}',  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€\n",
        "        f'/content/drive/MyDrive/Loras/{project_name}/dataset',  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€\n",
        "        f'/content/drive/MyDrive/Loras/{project_name}/output',  # å‡ºåŠ›ãƒ•ã‚©ãƒ«ãƒ€\n",
        "        f'/content/drive/MyDrive/Loras/{project_name}/config'  # ã‚³ãƒ³ãƒ•ã‚£ã‚°ãƒ•ã‚©ãƒ«ãƒ€\n",
        "    ]\n",
        "\n",
        "    # ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã™ã‚‹\n",
        "    for folder in folders_to_create:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "        print(f\"ğŸ“‚ ãƒ•ã‚©ãƒ«ãƒ€ {folder} ãŒä½œæˆã•ã‚Œã¾ã—ãŸ\")\n",
        "\n",
        "# é–¢æ•°ã‚’å®Ÿè¡Œ\n",
        "create_folders()\n",
        "\n",
        "print(\"ğŸŒGoogleDriveã¨ã®æ¥ç¶šãŒå®Œäº†ã—ã¾ã—ãŸï¼ ğŸŒ±Stepâ‹ã¸é€²ã‚“ã§ãã ã•ã„\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OglZzI_ujZq-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@title # ğŸŒ±ï¼œStep2ï¼ å­¦ç¿’ã‚’ã¯ã˜ã‚ã‚‹å‰ã®è¨­å®š\n",
        "\n",
        "#@markdown # ğŸ“”å­¦ç¿’ã«é–¢ã™ã‚‹è¨­å®šâŠâ·â‘¢\n",
        "#@markdown #ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
        "#@markdown #âŠã€ŒğŸ“šå­¦ç¿’ç”¨ã®ç”»åƒã€ï¼†ã€ŒğŸ“‘ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã€ã®æ ¼ç´\n",
        "#@markdown ### ã€€ãƒ»ã€ŒğŸ“‚datasetã€å†…ã«ã€Œå­¦ç¿’ç”¨ã®ç”»åƒã€ã¨ã€Œã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã€ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\n",
        "#@markdown ### ã€€â€»â—€ã®ãƒ•ã‚©ãƒ«ãƒ€ãƒªã‚¹ãƒˆãƒªã‚¹ãƒˆã‚’é–‹ã„ã¦ã€å¯¾è±¡ã®ã€ŒğŸ“‚datasetã€ã‚’å³ã‚¯ãƒªãƒƒã‚¯\n",
        "\n",
        "#@markdown #ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼\n",
        "\n",
        "#@markdown #â· ğŸ”€å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®æŒ‡å®š\n",
        "#@markdown ### ã€€ãƒ»GoogleDriveå†…ã«ã‚ã‚‹ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆCheckpointï¼‰ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\n",
        "checkpoint_model_path = \"/content/drive/MyDrive/Original_Lora/checkpoint/ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½ï½\"  # @param {type:\"string\"}\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®åˆ¤å®š\n",
        "model_url = checkpoint_model_path  # Google Driveä¸Šã®ãƒ‘ã‚¹ã‚’æŒ‡å®š\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
        "if not os.path.exists(model_url):\n",
        "    raise FileNotFoundError(f\"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_url}\")\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†\n",
        "model_file = model_url  # ãã®ã¾ã¾ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’åˆ©ç”¨\n",
        "\n",
        "# ãƒ‘ã‚¹ã‚„ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„ã‹ã©ã†ã‹ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ç°¡å˜ãªå‡ºåŠ›\n",
        "print(f\"ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹: {model_url}\")\n",
        "\n",
        "#@markdown #ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼ãƒ¼\n",
        "\n",
        "#@markdown #â‘¢ âš™ï¸å„ç¨®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆè¨­å®šçœç•¥ï¼‰\n",
        "#@markdown ### ã€€â™»ï¸ ğŸŒ±ãŠè©¦ã—ç‰ˆãªã®ã§å„ç¨®ã€ç§ã®ã‚ªã‚¹ã‚¹ãƒ¡å›ºå®šå€¤ã§è¨­å®šæ¸ˆã¿ã§ã™ã€‚\n",
        "# å›ºå®šã•ã‚ŒãŸå­¦ç¿’è¨­å®š\n",
        "resolution = 512  # è§£åƒåº¦ã‚’512ã«å›ºå®š\n",
        "flip_aug = False  # ã‚¿ã‚°ã®ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã‚’ã—ãªã„\n",
        "shuffle_tags = True  # ã‚¿ã‚°ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¯Trueã§å›ºå®š\n",
        "\n",
        "shuffle_caption = shuffle_tags\n",
        "activation_tags = 1  # ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã¯1ã«å›ºå®š\n",
        "keep_tokens = int(activation_tags)\n",
        "\n",
        "# å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã«é–¢ã™ã‚‹è¨­å®š\n",
        "num_repeats = 10  # ç¹°ã‚Šè¿”ã—å›æ•°ã¯10ã«å›ºå®š\n",
        "max_train_epochs = 10  # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’10ã«å›ºå®š\n",
        "max_train_steps = None  # ã‚¹ãƒ†ãƒƒãƒ—æ•°ã¯ä½¿ç”¨ã—ãªã„\n",
        "\n",
        "save_every_n_epochs = 10  # 10ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«ä¿å­˜\n",
        "keep_only_last_n_epochs = 10  # æœ€æ–°ã®10ã‚¨ãƒãƒƒã‚¯ã®ã¿ä¿æŒ\n",
        "\n",
        "train_batch_size = 2  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’2ã«å›ºå®š\n",
        "\n",
        "# å­¦ç¿’ç‡ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®è¨­å®š\n",
        "unet_lr = 5e-4  # UNetã®å­¦ç¿’ç‡ã‚’5e-4ã«å›ºå®š\n",
        "text_encoder_lr = 1e-4  # ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®å­¦ç¿’ç‡ã‚’1e-4ã«å›ºå®š\n",
        "\n",
        "lr_scheduler = \"cosine_with_restarts\"  # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã¯`cosine_with_restarts`ã«å›ºå®š\n",
        "lr_scheduler_num_cycles = 3  # ã‚µã‚¤ã‚¯ãƒ«æ•°ã‚’3ã«å›ºå®š\n",
        "lr_scheduler_power = 0  # ãƒãƒªãƒãƒŸã‚¢ãƒ«ã®ãƒ‘ãƒ¯ãƒ¼ã‚’0ã«å›ºå®š\n",
        "\n",
        "lr_warmup_ratio = 0.05  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¯”ç‡ã‚’5%ã«å›ºå®š\n",
        "lr_warmup_steps = 0  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ãƒ†ãƒƒãƒ—ã¯ä½¿ç”¨ã—ãªã„\n",
        "\n",
        "min_snr_gamma = True  # SNRã‚¬ãƒ³ãƒã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
        "min_snr_gamma_value = 5.0  # SNRã‚¬ãƒ³ãƒã®å€¤ã‚’5.0ã«å›ºå®š\n",
        "\n",
        "# LoRAã®ã‚¿ã‚¤ãƒ—ã¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®š\n",
        "lora_type = \"LoRA\"  # LoRAã‚¿ã‚¤ãƒ—ã‚’\"LoRA\"ã«å›ºå®š\n",
        "network_dim = 16  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¬¡å…ƒã‚’16ã«å›ºå®š\n",
        "network_alpha = 8  # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚¢ãƒ«ãƒ•ã‚¡ã‚’8ã«å›ºå®š\n",
        "\n",
        "# LoConã®è¿½åŠ è¨­å®š\n",
        "conv_dim = 8  # LoConã®ç•³ã¿è¾¼ã¿æ¬¡å…ƒã‚’8ã«å›ºå®š\n",
        "conv_alpha = 4  # LoConã®ã‚¢ãƒ«ãƒ•ã‚¡ã‚’4ã«å›ºå®š\n",
        "\n",
        "network_module = \"networks.lora\"\n",
        "network_args = None\n",
        "if lora_type.lower() == \"locon\":\n",
        "    network_args = [f\"conv_dim={conv_dim}\", f\"conv_alpha={conv_alpha}\"]\n",
        "\n",
        "#@markdown #ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼\n",
        "#@markdown # ğŸ‘Œ â†‘ã“ã“ã¾ã§è¨­å®šã§ããŸã‚‰æº–å‚™å®Œäº†â†‘\n",
        "#@markdown ğŸš©ğŸŒ±ï¼œStep2ï¼ã®ï¼ˆâ–¶ï¸ï¼‰å®Ÿè¡Œãƒœã‚¿ãƒ³ã‚’æŠ¼ã™ã¨ã€LoRAã®å­¦ç¿’ãŒé–‹å§‹ã•ã‚Œã¾ã™ã€‚ <p>\n",
        "\n",
        "\n",
        "\n",
        "# ã“ã“ã‹ã‚‰ä¸‹ãŒå­¦ç¿’ã‚³ãƒãƒ³ãƒ‰\n",
        "\n",
        "if optimizer.lower() == \"prodigy\" or \"dadapt\" in optimizer.lower():\n",
        "  if override_values_for_dadapt_and_prodigy:\n",
        "    unet_lr = 0.5\n",
        "    text_encoder_lr = 0.5\n",
        "    lr_scheduler = \"constant_with_warmup\"\n",
        "    lr_warmup_ratio = 0.05\n",
        "    network_alpha = network_dim\n",
        "\n",
        "  if not optimizer_args:\n",
        "    optimizer_args = [\"decouple=True\",\"weight_decay=0.01\",\"betas=[0.9,0.999]\"]\n",
        "    if optimizer == \"Prodigy\":\n",
        "      optimizer_args.extend([\"d_coef=2\",\"use_bias_correction=True\"])\n",
        "      if lr_warmup_ratio > 0:\n",
        "        optimizer_args.append(\"safeguard_warmup=True\")\n",
        "      else:\n",
        "        optimizer_args.append(\"safeguard_warmup=False\")\n",
        "\n",
        "root_dir = \"/content\" if COLAB else \"~/Loras\"\n",
        "deps_dir = os.path.join(root_dir, \"deps\")\n",
        "repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "\n",
        "if \"/Loras\" in folder_structure:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/Loras\") if COLAB else root_dir\n",
        "  log_folder    = os.path.join(main_dir, \"_logs\")\n",
        "  config_folder = os.path.join(main_dir, project_name)\n",
        "  images_folder = os.path.join(main_dir, project_name, \"dataset\")\n",
        "  output_folder = os.path.join(main_dir, project_name, \"output\")\n",
        "else:\n",
        "  main_dir      = os.path.join(root_dir, \"drive/MyDrive/lora_training\") if COLAB else root_dir\n",
        "  images_folder = os.path.join(main_dir, \"datasets\", project_name)\n",
        "  output_folder = os.path.join(main_dir, \"output\", project_name)\n",
        "  config_folder = os.path.join(main_dir, \"config\", project_name)\n",
        "  log_folder    = os.path.join(main_dir, \"log\")\n",
        "\n",
        "config_file = os.path.join(config_folder, \"training_config.toml\")\n",
        "dataset_config_file = os.path.join(config_folder, \"dataset_config.toml\")\n",
        "accelerate_config_file = os.path.join(repo_dir, \"accelerate_config/config.yaml\")\n",
        "\n",
        "def install_dependencies():\n",
        "  os.chdir(root_dir)\n",
        "  !git clone {SOURCE} {repo_dir}\n",
        "  os.chdir(repo_dir)\n",
        "  if COMMIT:\n",
        "    !git reset --hard {COMMIT}\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/train_network_wrapper.py -q -O train_network_wrapper.py\n",
        "  !wget https://raw.githubusercontent.com/hollowstrawberry/kohya-colab/main/dracula.py -q -O dracula.py\n",
        "\n",
        "  !apt -y update -qq\n",
        "  !apt -y install aria2 -qq\n",
        "  !pip install accelerate==0.25.0 transformers==4.36.2 diffusers[torch]==0.25.0 ftfy==6.1.1 \\\n",
        "    opencv-python==4.8.1.78 einops==0.7.0 pytorch-lightning==1.9.0 bitsandbytes==0.43.0 \\\n",
        "    prodigyopt==1.0 lion-pytorch==0.0.6 tensorboard safetensors==0.4.2 altair==4.2.2 \\\n",
        "    easygui==0.98.3 toml==0.10.2 voluptuous==0.13.1 huggingface-hub==0.20.1 imagesize==1.4.1 \\\n",
        "    rich==13.7.1 torch==2.4.1+cu121 triton\n",
        "  !pip install -e .\n",
        "  if XFORMERS:\n",
        "    !pip install xformers==0.0.28.post1\n",
        "\n",
        "  # patch kohya for minor stuff\n",
        "  if COLAB:\n",
        "    !sed -i \"s@cpu@cuda@\" library/model_util.py # low ram\n",
        "  if LOAD_TRUNCATED_IMAGES:\n",
        "    !sed -i 's/from PIL import Image/from PIL import Image, ImageFile\\nImageFile.LOAD_TRUNCATED_IMAGES=True/g' library/train_util.py # fix truncated jpegs error\n",
        "  if BETTER_EPOCH_NAMES:\n",
        "    !sed -i 's/{:06d}/{:02d}/g' library/train_util.py # make epoch names shorter\n",
        "    !sed -i 's/\".\" + args.save_model_as)/\"-{:02d}.\".format(num_train_epochs) + args.save_model_as)/g' train_network.py # name of the last epoch will match the rest\n",
        "\n",
        "  from accelerate.utils import write_basic_config\n",
        "  if not os.path.exists(accelerate_config_file):\n",
        "    write_basic_config(save_location=accelerate_config_file)\n",
        "\n",
        "  os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "  os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
        "  os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "\n",
        "def validate_dataset():\n",
        "    global lr_warmup_steps, lr_warmup_ratio, caption_extension, keep_tokens, keep_tokens_weight, weighted_captions, adjust_tags\n",
        "    supported_types = (\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\")\n",
        "\n",
        "    print(\"\\nğŸ’¿ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç¢ºèªä¸­...\")\n",
        "\n",
        "    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã®ãƒã‚§ãƒƒã‚¯\n",
        "    if not project_name.strip() or any(c in project_name for c in \" .()\\\"'\\\\/\"):\n",
        "        print(\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: æœ‰åŠ¹ãªãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚\")\n",
        "        return\n",
        "\n",
        "    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€ã®å­˜åœ¨ç¢ºèª\n",
        "    if not os.path.exists(images_folder):\n",
        "        print(f\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚©ãƒ«ãƒ€ {images_folder} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚\")\n",
        "        return\n",
        "    else:\n",
        "        print(f\"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚©ãƒ«ãƒ€ {images_folder} ã¯å­˜åœ¨ã—ã¾ã™ã€‚\")\n",
        "\n",
        "    if custom_dataset:\n",
        "        try:\n",
        "            datconf = toml.loads(custom_dataset)\n",
        "            datasets = [d for d in datconf[\"datasets\"][0][\"subsets\"]]\n",
        "        except:\n",
        "            print(f\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç„¡åŠ¹ã‹ã‚¨ãƒ©ãƒ¼ã‚’å«ã‚“ã§ã„ã¾ã™ï¼ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
        "            return\n",
        "        reg = [d.get(\"image_dir\") for d in datasets if d.get(\"is_reg\", False)]\n",
        "        datasets_dict = {d[\"image_dir\"]: d[\"num_repeats\"] for d in datasets}\n",
        "        folders = datasets_dict.keys()\n",
        "        files = [f for folder in folders for f in os.listdir(folder)]\n",
        "        images_repeats = {folder: (len([f for f in os.listdir(folder) if f.lower().endswith(supported_types)]), datasets_dict[folder]) for folder in folders}\n",
        "    else:\n",
        "        reg = []\n",
        "        folders = [images_folder]\n",
        "        files = os.listdir(images_folder)\n",
        "        images_repeats = {images_folder: (len([f for f in files if f.lower().endswith(supported_types)]), num_repeats)}\n",
        "\n",
        "    for folder in folders:\n",
        "        if not os.path.exists(folder):\n",
        "            print(f\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚©ãƒ«ãƒ€ {folder.replace('/content/drive/', '')} ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚\")\n",
        "            return\n",
        "    for folder, (img, rep) in images_repeats.items():\n",
        "        if not img:\n",
        "            print(f\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: ãƒ•ã‚©ãƒ«ãƒ€ {folder.replace('/content/drive/', '')} ãŒç©ºã§ã™ã€‚\")\n",
        "            return\n",
        "    for f in files:\n",
        "        if not f.lower().endswith((\".txt\", \".npz\")) and not f.lower().endswith(supported_types):\n",
        "            print(f\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã«ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã™: \\\"{f}\\\"ã€‚ä¸­æ­¢ã—ã¾ã™ã€‚\")\n",
        "            return\n",
        "\n",
        "    if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "        caption_extension = \"\"\n",
        "    if continue_from_lora and not (continue_from_lora.endswith(\".safetensors\") and os.path.exists(continue_from_lora)):\n",
        "        print(f\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: æ—¢å­˜ã®Loraã¸ã®ç„¡åŠ¹ãªãƒ‘ã‚¹ã€‚ä¾‹: /content/drive/MyDrive/Loras/example.safetensors\")\n",
        "        return\n",
        "\n",
        "    pre_steps_per_epoch = sum(img * rep for (img, rep) in images_repeats.values())\n",
        "    steps_per_epoch = pre_steps_per_epoch / train_batch_size\n",
        "    total_steps = max_train_steps or int(max_train_epochs * steps_per_epoch)\n",
        "    estimated_epochs = int(total_steps / steps_per_epoch)\n",
        "    lr_warmup_steps = int(total_steps * lr_warmup_ratio)\n",
        "\n",
        "    for folder, (img, rep) in images_repeats.items():\n",
        "        print(\"ğŸ“\" + folder.replace(\"/content/drive/\", \"\") + (\" (æ­£è¦åŒ–)\" if folder in reg else \"\"))\n",
        "        print(f\"ğŸ“ˆ {img}æšã®ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã€{rep}å›ã®ç¹°ã‚Šè¿”ã—ã§ã€åˆè¨ˆ {img * rep} ã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚\")\n",
        "    print(f\"ğŸ“‰ {pre_steps_per_epoch} ã‚¹ãƒ†ãƒƒãƒ—ã‚’ {train_batch_size} ãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‰²ã‚Šã€1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Š {steps_per_epoch} ã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚\")\n",
        "    if max_train_epochs:\n",
        "        print(f\"ğŸŒ€ åˆè¨ˆã§ {total_steps} ã‚¹ãƒ†ãƒƒãƒ—ã®å­¦ç¿’ã‚’ {max_train_epochs} ã‚¨ãƒãƒƒã‚¯è¡Œã„ã¾ã™ã€‚\")\n",
        "    else:\n",
        "        print(f\"ğŸŒ€ åˆè¨ˆã§ {total_steps} ã‚¹ãƒ†ãƒƒãƒ—ã‚’ {estimated_epochs} ã‚¨ãƒãƒƒã‚¯ã§åˆ†å‰²ã—ã¦å­¦ç¿’ã—ã¾ã™ã€‚\")\n",
        "\n",
        "    if total_steps > 10000:\n",
        "        print(\"ğŸ‘€ ã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒƒãƒ—ãŒå¤šã™ãã¾ã™ã€‚ä½•ã‹é–“é•ã„ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ä¸­æ­¢ã—ã¾ã™...\")\n",
        "        return\n",
        "\n",
        "    if adjust_tags:\n",
        "        print(f\"\\nğŸ“ é‡ã¿ä»˜ãã‚¿ã‚°: {'ON' if weighted_captions else 'OFF'}\")\n",
        "        if weighted_captions:\n",
        "            print(f\"ğŸ“ {keep_tokens} ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚°ã« {keep_tokens_weight} ã®é‡ã¿ã‚’é©ç”¨ã—ã¾ã™ã€‚\")\n",
        "        print(\"ğŸ“ ã‚¿ã‚°ã‚’èª¿æ•´ä¸­...\")\n",
        "        adjust_weighted_tags(folders, keep_tokens, keep_tokens_weight, weighted_captions)\n",
        "\n",
        "    return True\n",
        "\n",
        "def adjust_weighted_tags(folders, keep_tokens: int, keep_tokens_weight: float, weighted_captions: bool):\n",
        "  weighted_tag = re.compile(r\"\\((.+?):[.\\d]+\\)(,|$)\")\n",
        "  for folder in folders:\n",
        "    for txt in [f for f in os.listdir(folder) if f.lower().endswith(\".txt\")]:\n",
        "      with open(os.path.join(folder, txt), 'r') as f:\n",
        "        content = f.read()\n",
        "      # reset previous changes\n",
        "      content = content.replace('\\\\', '')\n",
        "      content = weighted_tag.sub(r'\\1\\2', content)\n",
        "      if weighted_captions:\n",
        "        # re-apply changes\n",
        "        content = content.replace(r'(', r'\\(').replace(r')', r'\\)').replace(r':', r'\\:')\n",
        "        if keep_tokens_weight > 1:\n",
        "          tags = [s.strip() for s in content.split(\",\")]\n",
        "          for i in range(min(keep_tokens, len(tags))):\n",
        "            tags[i] = f'({tags[i]}:{keep_tokens_weight})'\n",
        "          content = \", \".join(tags)\n",
        "      with open(os.path.join(folder, txt), 'w') as f:\n",
        "        f.write(content)\n",
        "\n",
        "def create_config():\n",
        "    global dataset_config_file, config_file, model_file\n",
        "\n",
        "    # ç”»åƒãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ã‚’å–å¾—\n",
        "    files = os.listdir(images_folder)  # ã“ã“ã§images_folderå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—\n",
        "\n",
        "    # caption_extensionã®å®šç¾©\n",
        "    if not [txt for txt in files if txt.lower().endswith(\".txt\")]:\n",
        "        caption_extension = \"\"\n",
        "    else:\n",
        "        caption_extension = \".txt\"\n",
        "\n",
        "    if override_config_file:\n",
        "        config_file = override_config_file\n",
        "        print(f\"\\nğŸ™† Using custom config file {config_file}\")\n",
        "    else:\n",
        "        config_dict = {\n",
        "            \"additional_network_arguments\": {\n",
        "                \"unet_lr\": unet_lr,\n",
        "                \"text_encoder_lr\": text_encoder_lr,\n",
        "                \"network_dim\": network_dim,\n",
        "                \"network_alpha\": network_alpha,\n",
        "                \"network_module\": network_module,\n",
        "                \"network_args\": network_args,\n",
        "                \"network_train_unet_only\": True if text_encoder_lr == 0 else None,\n",
        "                \"network_weights\": continue_from_lora if continue_from_lora else None\n",
        "            },\n",
        "            \"optimizer_arguments\": {\n",
        "                \"learning_rate\": unet_lr,\n",
        "                \"lr_scheduler\": lr_scheduler,\n",
        "                \"lr_scheduler_num_cycles\": lr_scheduler_num_cycles if lr_scheduler == \"cosine_with_restarts\" else None,\n",
        "                \"lr_scheduler_power\": lr_scheduler_power if lr_scheduler == \"polynomial\" else None,\n",
        "                \"lr_warmup_steps\": lr_warmup_steps if lr_scheduler != \"constant\" else None,\n",
        "                \"optimizer_type\": optimizer,\n",
        "                \"optimizer_args\": optimizer_args if optimizer_args else None,\n",
        "            },\n",
        "            \"training_arguments\": {\n",
        "                \"max_train_steps\": max_train_steps,\n",
        "                \"max_train_epochs\": max_train_epochs,\n",
        "                \"save_every_n_epochs\": save_every_n_epochs,\n",
        "                \"save_last_n_epochs\": keep_only_last_n_epochs,\n",
        "                \"train_batch_size\": train_batch_size,\n",
        "                \"noise_offset\": None,\n",
        "                \"clip_skip\": 2,\n",
        "                \"min_snr_gamma\": min_snr_gamma_value,\n",
        "                \"weighted_captions\": weighted_captions,\n",
        "                \"seed\": 42,\n",
        "                \"max_token_length\": 225,\n",
        "                \"xformers\": XFORMERS,\n",
        "                \"lowram\": COLAB,\n",
        "                \"max_data_loader_n_workers\": 8,\n",
        "                \"persistent_data_loader_workers\": True,\n",
        "                \"save_precision\": \"fp16\",\n",
        "                \"mixed_precision\": \"fp16\",\n",
        "                \"output_dir\": output_folder,\n",
        "                \"logging_dir\": log_folder,\n",
        "                \"output_name\": project_name,\n",
        "                \"log_prefix\": project_name,\n",
        "            },\n",
        "            \"model_arguments\": {\n",
        "                \"pretrained_model_name_or_path\": model_file,\n",
        "                \"v2\": custom_model_is_based_on_sd2,\n",
        "                \"v_parameterization\": True if custom_model_is_based_on_sd2 else None,\n",
        "            },\n",
        "            \"saving_arguments\": {\n",
        "                \"save_model_as\": \"safetensors\",\n",
        "            },\n",
        "            \"dreambooth_arguments\": {\n",
        "                \"prior_loss_weight\": 1.0,\n",
        "            },\n",
        "            \"dataset_arguments\": {\n",
        "                \"cache_latents\": True,\n",
        "                \"caption_extension\": caption_extension,  # ã“ã“ã§è¿½åŠ \n",
        "            },\n",
        "        }\n",
        "\n",
        "        for key in config_dict:\n",
        "            if isinstance(config_dict[key], dict):\n",
        "                config_dict[key] = {k: v for k, v in config_dict[key].items() if v is not None}\n",
        "\n",
        "        with open(config_file, \"w\") as f:\n",
        "            f.write(toml.dumps(config_dict))\n",
        "        print(f\"\\nğŸ“„ Config saved to {config_file}\")\n",
        "\n",
        "    if override_dataset_config_file:\n",
        "        dataset_config_file = override_dataset_config_file\n",
        "        print(f\"ğŸ™† Using custom dataset config file {dataset_config_file}\")\n",
        "    else:\n",
        "        dataset_config_dict = {\n",
        "            \"general\": {\n",
        "                \"resolution\": resolution,\n",
        "                \"shuffle_caption\": shuffle_caption,\n",
        "                \"keep_tokens\": keep_tokens,\n",
        "                \"flip_aug\": flip_aug,\n",
        "                \"caption_extension\": caption_extension,\n",
        "                \"enable_bucket\": True,\n",
        "                \"bucket_reso_steps\": 64,\n",
        "                \"bucket_no_upscale\": False,\n",
        "                \"min_bucket_reso\": 320 if resolution > 640 else 256,\n",
        "                \"max_bucket_reso\": 1280 if resolution > 640 else 1024,\n",
        "            },\n",
        "            \"datasets\": toml.loads(custom_dataset)[\"datasets\"] if custom_dataset else [\n",
        "                {\n",
        "                    \"subsets\": [\n",
        "                        {\n",
        "                            \"num_repeats\": num_repeats,\n",
        "                            \"image_dir\": images_folder,\n",
        "                            \"class_tokens\": None if caption_extension else project_name\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        for key in dataset_config_dict:\n",
        "            if isinstance(dataset_config_dict[key], dict):\n",
        "                dataset_config_dict[key] = {k: v for k, v in dataset_config_dict[key].items() if v is not None}\n",
        "\n",
        "        with open(dataset_config_file, \"w\") as f:\n",
        "            f.write(toml.dumps(dataset_config_dict))\n",
        "        print(f\"ğŸ“„ Dataset config saved to {dataset_config_file}\")\n",
        "\n",
        "\n",
        "\n",
        "def download_model():\n",
        "    global old_model_url, model_url, model_file\n",
        "    real_model_url = model_url.strip()\n",
        "\n",
        "    # Google Driveã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ãã®ã¾ã¾ä½¿ç”¨ã™ã‚‹å ´åˆ\n",
        "    if real_model_url.startswith(\"/content/drive/\"):\n",
        "        print(f\"ğŸ”€ Using model directly from Google Drive: {real_model_url}\")\n",
        "        model_file = real_model_url\n",
        "        return True\n",
        "    else:\n",
        "        # é€šå¸¸ã®HTTP/HTTPSãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†\n",
        "        model_file = \"/content/downloaded_model.safetensors\"\n",
        "        if os.path.exists(model_file):\n",
        "            os.remove(model_file)\n",
        "\n",
        "        if m := re.search(r\"(?:https?://)?(?:www\\.)?huggingface\\.co/[^/]+/[^/]+/blob\", model_url):\n",
        "            real_model_url = real_model_url.replace(\"blob\", \"resolve\")\n",
        "        elif m := re.search(r\"(?:https?://)?(?:www\\\\.)?civitai\\.com/models/([0-9]+)(/[A-Za-z0-9-_]+)?\", model_url):\n",
        "            if m.group(2):\n",
        "                model_file = f\"/content{m.group(2)}.safetensors\"\n",
        "            if m := re.search(r\"modelVersionId=([0-9]+)\", model_url):\n",
        "                real_model_url = f\"https://civitai.com/api/download/models/{m.group(1)}\"\n",
        "            else:\n",
        "                raise ValueError(\"Invalid civitai link or missing modelVersionId.\")\n",
        "\n",
        "        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†\n",
        "        !aria2c \"{real_model_url}\" --console-log-level=warn -c -s 16 -x 16 -k 10M -d / -o \"{model_file}\"\n",
        "\n",
        "    # safetensorså½¢å¼ã‹ã©ã†ã‹ã‚’ç¢ºèª\n",
        "    if model_file.lower().endswith(\".safetensors\"):\n",
        "        from safetensors.torch import load_file as load_safetensors\n",
        "        try:\n",
        "            test = load_safetensors(model_file)\n",
        "            del test\n",
        "        except:\n",
        "            new_model_file = os.path.splitext(model_file)[0]+\".ckpt\"\n",
        "            shutil.move(model_file, new_model_file)\n",
        "            model_file = new_model_file\n",
        "            print(f\"Renamed model to {os.path.splitext(model_file)[0]}.ckpt\")\n",
        "\n",
        "    # .ckptå½¢å¼ã®å ´åˆ\n",
        "    if model_file.lower().endswith(\".ckpt\"):\n",
        "        from torch import load as load_ckpt\n",
        "        try:\n",
        "            test = load_ckpt(model_file)\n",
        "            del test\n",
        "        except:\n",
        "            return False\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "def main():\n",
        "  global dependencies_installed\n",
        "\n",
        "  if COLAB and not os.path.exists('/content/drive'):\n",
        "    from google.colab import drive\n",
        "    print(\"ğŸ“‚ Connecting to Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "  for dir in (main_dir, deps_dir, repo_dir, log_folder, images_folder, output_folder, config_folder):\n",
        "    os.makedirs(dir, exist_ok=True)\n",
        "\n",
        "  if not validate_dataset():\n",
        "    return\n",
        "\n",
        "  if not dependencies_installed:\n",
        "    print(\"\\nğŸ  Installing dependencies...\\n\")\n",
        "    t0 = time()\n",
        "    install_dependencies()\n",
        "    t1 = time()\n",
        "    dependencies_installed = True\n",
        "    print(f\"\\nâœ… Installation finished in {int(t1-t0)} seconds.\")\n",
        "  else:\n",
        "    print(\"\\nâœ… Dependencies already installed.\")\n",
        "\n",
        "  if old_model_url != model_url or not model_file or not os.path.exists(model_file):\n",
        "    print(\"\\nğŸ”€ Downloading model...\")\n",
        "    if not download_model():\n",
        "      print(\"\\nğŸ‘€ Error: The model you selected is invalid or corrupted, or couldn't be downloaded. You can use a civitai or huggingface link, or any direct download link.\")\n",
        "      return\n",
        "    print()\n",
        "  else:\n",
        "    print(\"\\nğŸ”€ Model already downloaded.\\n\")\n",
        "\n",
        "  create_config()\n",
        "\n",
        "  print(\"\\nğŸš© Starting trainer...\\n\")\n",
        "  os.chdir(repo_dir)\n",
        "\n",
        "  !accelerate launch --config_file={accelerate_config_file} --num_cpu_threads_per_process=1 train_network_wrapper.py --dataset_config={dataset_config_file} --config_file={config_file}\n",
        "\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}